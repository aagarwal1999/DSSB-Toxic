{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Improved LSTM baseline\n",
    "\n",
    "This kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "import keras.layers\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "EMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TRAIN2_DATA_FILE=f'{path}train2.csv'\n",
    "TRAIN3_DATA_FILE=f'{path}train3.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'\n",
    "TRAIN4_DATA_FILE=f'{path}train4.csv'\n",
    "TEST4_DATA_FILE=f'{path}test4.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN2_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "train3 = pd.read_csv(TRAIN3_DATA_FILE)\n",
    "test4 = pd.read_csv(TEST4_DATA_FILE)\n",
    "train4 = pd.read_csv(TRAIN4_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>explanation why the edit make under my usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>d aww he match this background colour i am see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>hey man i am really not try to edit war it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>more i cannot make any real suggestions on imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\r\\n\\r\\nCongratulations from me as well, use ...</td>\n",
       "      <td>congratulations from me as well use the tool w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>sorry if the word nonsense be offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>alignment on this subject and which be contrar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"\\r\\nFair use rationale for Image:Wonju.jpg\\r\\...</td>\n",
       "      <td>fair use rationale for image wonju jpg thank f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq \\r\\n\\r\\nbe a man and lets discuss it-maybe...</td>\n",
       "      <td>bbq be a man and let discuss it maybe over the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is i...</td>\n",
       "      <td>hey what be it talk what be it an exclusive gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>before you start throw accusations and warn at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>oh and the girl above start her arguments with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"\\r\\n\\r\\nJuelz Santanas Age\\r\\n\\r\\nIn 2002, Ju...</td>\n",
       "      <td>juelz santanas age in 2002 juelz santana be 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\r\\n\\r\\nDon't look, come or think of comm...</td>\n",
       "      <td>bye do not look come or think of comming back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski</td>\n",
       "      <td>redirect talk voydan pop georgiev chernodrinski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Mitsurugi point made no sense - why not ar...</td>\n",
       "      <td>the mitsurugi point make no sense why not argu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Don't mean to bother you \\r\\n\\r\\nI see that yo...</td>\n",
       "      <td>do not mean to bother you i see that you are w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"\\r\\n\\r\\n Regarding your recent edits \\r\\n\\r\\n...</td>\n",
       "      <td>regard your recent edit once again please read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"\\r\\nGood to know. About me, yeah, I'm studyin...</td>\n",
       "      <td>good to know about me yeah i am study now deepu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"\\r\\n\\r\\n Snowflakes are NOT always symmetrica...</td>\n",
       "      <td>snowflakes be not always symmetrical under geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"\\r\\n\\r\\n The Signpost: 24 September 2012 \\r\\n...</td>\n",
       "      <td>the signpost 24 september 2012 read this signp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"\\r\\n\\r\\nRe-considering 1st paragraph edit?\\r\\...</td>\n",
       "      <td>re considering 1st paragraph edit i do not und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Radial symmetry \\r\\n\\r\\nSeveral now extinct li...</td>\n",
       "      <td>radial symmetry several now extinct lineages i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>There's no need to apologize. A Wikipedia arti...</td>\n",
       "      <td>there is no need to apologize a wikipedia arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Yes, because the mother of the child in the ca...</td>\n",
       "      <td>yes because the mother of the child in the cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"\\r\\nOk. But it will take a bit of work but I ...</td>\n",
       "      <td>ok but it will take a bite of work but i canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"== A barnstar for you! ==\\r\\n\\r\\n  The Real L...</td>\n",
       "      <td>a barnstar for you the real life barnstar let ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>Your absurd edits \\r\\n\\r\\nYour absurd edits on...</td>\n",
       "      <td>your absurd edit your absurd edit on great whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159542</th>\n",
       "      <td>maybe he's got better things to do than spend ...</td>\n",
       "      <td>maybe he is get better things to do than spend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159543</th>\n",
       "      <td>scrap that, it does meet criteria and its gone...</td>\n",
       "      <td>scrap that it do meet criteria and its go to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159544</th>\n",
       "      <td>You could do worse.</td>\n",
       "      <td>you could do worse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159545</th>\n",
       "      <td>, 7 March 2011 (UTC)\\r\\nAre you also User:Bmat...</td>\n",
       "      <td>7 march 2011 utc be you also user bmattson wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159546</th>\n",
       "      <td>\"\\r\\n\\r\\nHey listen don't you ever!!!! Delete ...</td>\n",
       "      <td>hey listen do not you ever delete my edit ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159547</th>\n",
       "      <td>Thank you very, very much.  ·✆</td>\n",
       "      <td>thank you very very much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159548</th>\n",
       "      <td>Talkback: 15 September 2012</td>\n",
       "      <td>talkback 15 september 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159549</th>\n",
       "      <td>2005 (UTC)\\r\\n 06:35, 31 Mar</td>\n",
       "      <td>2005 utc 06 35 31 mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159550</th>\n",
       "      <td>i agree/ on another note lil wayne is a talent...</td>\n",
       "      <td>i agree on another note lil wayne be a talentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159551</th>\n",
       "      <td>While about half the references are from BYU-I...</td>\n",
       "      <td>while about half the reference be from byu i t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159552</th>\n",
       "      <td>Prague Spring \\r\\n\\r\\nI think that Prague Spri...</td>\n",
       "      <td>prague spring i think that prague spring deser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159553</th>\n",
       "      <td>I see this as having been merged; undoing one ...</td>\n",
       "      <td>i see this as have be merge undo one side of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>and i'm going to keep posting the stuff u dele...</td>\n",
       "      <td>and i am go to keep post the stuff u delete un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159555</th>\n",
       "      <td>\"\\r\\n\\r\\nHow come when you download that MP3 i...</td>\n",
       "      <td>how come when you download that mp3 it is titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159556</th>\n",
       "      <td>I'll be on IRC, too, if you have a more specif...</td>\n",
       "      <td>i will be on irc too if you have a more specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159557</th>\n",
       "      <td>It is my opinion that that happens to be off-t...</td>\n",
       "      <td>it be my opinion that that happen to be off to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159558</th>\n",
       "      <td>Please stop removing content from Wikipedia; i...</td>\n",
       "      <td>please stop remove content from wikipedia it b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159559</th>\n",
       "      <td>Image:Barack-obama-mother.jpg listed for delet...</td>\n",
       "      <td>image barack obama mother jpg list for deletio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159560</th>\n",
       "      <td>\"Editing of article without Consensus &amp; Remova...</td>\n",
       "      <td>edit of article without consensus removal of c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159561</th>\n",
       "      <td>\"\\r\\nNo he did not, read it again (I would hav...</td>\n",
       "      <td>no he do not read it again i would have think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159562</th>\n",
       "      <td>\"\\r\\n Auto guides and the motoring press are n...</td>\n",
       "      <td>auto guide and the motor press be not good sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159563</th>\n",
       "      <td>\"\\r\\nplease identify what part of BLP applies ...</td>\n",
       "      <td>please identify what part of blp apply because...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159564</th>\n",
       "      <td>Catalan independentism is the social movement ...</td>\n",
       "      <td>catalan independentism be the social movement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159565</th>\n",
       "      <td>The numbers in parentheses are the additional ...</td>\n",
       "      <td>the number in parentheses be the additional de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>and for the second time of ask when your view ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\r\\n\\r\\nThat...</td>\n",
       "      <td>you should be ashamed of yourself that be a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\r\\n\\r\\nUmm, theres no actual article ...</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>and it look like it be actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\r\\nAnd ... I really don't think you understa...</td>\n",
       "      <td>and i really do not think you understand i com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  \\\n",
       "0       Explanation\\r\\nWhy the edits made under my use...   \n",
       "1       D'aww! He matches this background colour I'm s...   \n",
       "2       Hey man, I'm really not trying to edit war. It...   \n",
       "3       \"\\r\\nMore\\r\\nI can't make any real suggestions...   \n",
       "4       You, sir, are my hero. Any chance you remember...   \n",
       "5       \"\\r\\n\\r\\nCongratulations from me as well, use ...   \n",
       "6            COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7       Your vandalism to the Matt Shirvington article...   \n",
       "8       Sorry if the word 'nonsense' was offensive to ...   \n",
       "9       alignment on this subject and which are contra...   \n",
       "10      \"\\r\\nFair use rationale for Image:Wonju.jpg\\r\\...   \n",
       "11      bbq \\r\\n\\r\\nbe a man and lets discuss it-maybe...   \n",
       "12      Hey... what is it..\\r\\n@ | talk .\\r\\nWhat is i...   \n",
       "13      Before you start throwing accusations and warn...   \n",
       "14      Oh, and the girl above started her arguments w...   \n",
       "15      \"\\r\\n\\r\\nJuelz Santanas Age\\r\\n\\r\\nIn 2002, Ju...   \n",
       "16      Bye! \\r\\n\\r\\nDon't look, come or think of comm...   \n",
       "17       REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski   \n",
       "18      The Mitsurugi point made no sense - why not ar...   \n",
       "19      Don't mean to bother you \\r\\n\\r\\nI see that yo...   \n",
       "20      \"\\r\\n\\r\\n Regarding your recent edits \\r\\n\\r\\n...   \n",
       "21      \"\\r\\nGood to know. About me, yeah, I'm studyin...   \n",
       "22      \"\\r\\n\\r\\n Snowflakes are NOT always symmetrica...   \n",
       "23      \"\\r\\n\\r\\n The Signpost: 24 September 2012 \\r\\n...   \n",
       "24      \"\\r\\n\\r\\nRe-considering 1st paragraph edit?\\r\\...   \n",
       "25      Radial symmetry \\r\\n\\r\\nSeveral now extinct li...   \n",
       "26      There's no need to apologize. A Wikipedia arti...   \n",
       "27      Yes, because the mother of the child in the ca...   \n",
       "28      \"\\r\\nOk. But it will take a bit of work but I ...   \n",
       "29      \"== A barnstar for you! ==\\r\\n\\r\\n  The Real L...   \n",
       "...                                                   ...   \n",
       "159541  Your absurd edits \\r\\n\\r\\nYour absurd edits on...   \n",
       "159542  maybe he's got better things to do than spend ...   \n",
       "159543  scrap that, it does meet criteria and its gone...   \n",
       "159544                                You could do worse.   \n",
       "159545  , 7 March 2011 (UTC)\\r\\nAre you also User:Bmat...   \n",
       "159546  \"\\r\\n\\r\\nHey listen don't you ever!!!! Delete ...   \n",
       "159547                     Thank you very, very much.  ·✆   \n",
       "159548                        Talkback: 15 September 2012   \n",
       "159549                       2005 (UTC)\\r\\n 06:35, 31 Mar   \n",
       "159550  i agree/ on another note lil wayne is a talent...   \n",
       "159551  While about half the references are from BYU-I...   \n",
       "159552  Prague Spring \\r\\n\\r\\nI think that Prague Spri...   \n",
       "159553  I see this as having been merged; undoing one ...   \n",
       "159554  and i'm going to keep posting the stuff u dele...   \n",
       "159555  \"\\r\\n\\r\\nHow come when you download that MP3 i...   \n",
       "159556  I'll be on IRC, too, if you have a more specif...   \n",
       "159557  It is my opinion that that happens to be off-t...   \n",
       "159558  Please stop removing content from Wikipedia; i...   \n",
       "159559  Image:Barack-obama-mother.jpg listed for delet...   \n",
       "159560  \"Editing of article without Consensus & Remova...   \n",
       "159561  \"\\r\\nNo he did not, read it again (I would hav...   \n",
       "159562  \"\\r\\n Auto guides and the motoring press are n...   \n",
       "159563  \"\\r\\nplease identify what part of BLP applies ...   \n",
       "159564  Catalan independentism is the social movement ...   \n",
       "159565  The numbers in parentheses are the additional ...   \n",
       "159566  \":::::And for the second time of asking, when ...   \n",
       "159567  You should be ashamed of yourself \\r\\n\\r\\nThat...   \n",
       "159568  Spitzer \\r\\n\\r\\nUmm, theres no actual article ...   \n",
       "159569  And it looks like it was actually you who put ...   \n",
       "159570  \"\\r\\nAnd ... I really don't think you understa...   \n",
       "\n",
       "                                                  cleaned  \n",
       "0       explanation why the edit make under my usernam...  \n",
       "1       d aww he match this background colour i am see...  \n",
       "2       hey man i am really not try to edit war it is ...  \n",
       "3       more i cannot make any real suggestions on imp...  \n",
       "4       you sir be my hero any chance you remember wha...  \n",
       "5       congratulations from me as well use the tool w...  \n",
       "6            cocksucker before you piss around on my work  \n",
       "7       your vandalism to the matt shirvington article...  \n",
       "8       sorry if the word nonsense be offensive to you...  \n",
       "9       alignment on this subject and which be contrar...  \n",
       "10      fair use rationale for image wonju jpg thank f...  \n",
       "11      bbq be a man and let discuss it maybe over the...  \n",
       "12      hey what be it talk what be it an exclusive gr...  \n",
       "13      before you start throw accusations and warn at...  \n",
       "14      oh and the girl above start her arguments with...  \n",
       "15      juelz santanas age in 2002 juelz santana be 18...  \n",
       "16      bye do not look come or think of comming back ...  \n",
       "17        redirect talk voydan pop georgiev chernodrinski  \n",
       "18      the mitsurugi point make no sense why not argu...  \n",
       "19      do not mean to bother you i see that you are w...  \n",
       "20      regard your recent edit once again please read...  \n",
       "21        good to know about me yeah i am study now deepu  \n",
       "22      snowflakes be not always symmetrical under geo...  \n",
       "23      the signpost 24 september 2012 read this signp...  \n",
       "24      re considering 1st paragraph edit i do not und...  \n",
       "25      radial symmetry several now extinct lineages i...  \n",
       "26      there is no need to apologize a wikipedia arti...  \n",
       "27      yes because the mother of the child in the cas...  \n",
       "28      ok but it will take a bite of work but i canno...  \n",
       "29      a barnstar for you the real life barnstar let ...  \n",
       "...                                                   ...  \n",
       "159541  your absurd edit your absurd edit on great whi...  \n",
       "159542  maybe he is get better things to do than spend...  \n",
       "159543  scrap that it do meet criteria and its go to d...  \n",
       "159544                                 you could do worse  \n",
       "159545  7 march 2011 utc be you also user bmattson wha...  \n",
       "159546  hey listen do not you ever delete my edit ever...  \n",
       "159547                           thank you very very much  \n",
       "159548                         talkback 15 september 2012  \n",
       "159549                              2005 utc 06 35 31 mar  \n",
       "159550  i agree on another note lil wayne be a talentl...  \n",
       "159551  while about half the reference be from byu i t...  \n",
       "159552  prague spring i think that prague spring deser...  \n",
       "159553  i see this as have be merge undo one side of a...  \n",
       "159554  and i am go to keep post the stuff u delete un...  \n",
       "159555  how come when you download that mp3 it is titl...  \n",
       "159556  i will be on irc too if you have a more specif...  \n",
       "159557  it be my opinion that that happen to be off to...  \n",
       "159558  please stop remove content from wikipedia it b...  \n",
       "159559  image barack obama mother jpg list for deletio...  \n",
       "159560  edit of article without consensus removal of c...  \n",
       "159561  no he do not read it again i would have think ...  \n",
       "159562  auto guide and the motor press be not good sou...  \n",
       "159563  please identify what part of blp apply because...  \n",
       "159564  catalan independentism be the social movement ...  \n",
       "159565  the number in parentheses be the additional de...  \n",
       "159566  and for the second time of ask when your view ...  \n",
       "159567  you should be ashamed of yourself that be a ho...  \n",
       "159568  spitzer umm theres no actual article for prost...  \n",
       "159569  and it look like it be actually you who put on...  \n",
       "159570  and i really do not think you understand i com...  \n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train4[['comment_text','cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train4\n",
    "test = test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# notClean_array = 1 * ((train[\"toxic\"] == 1) | (train[\"obscene\"] == 1) | (train[\"threat\"] == 1) | (train['insult'] == 1) | (train['identity_hate'] == 1))\n",
    "# clean = train[notClean_array == 0]\n",
    "# just_toxic = train[notClean_array == 1]\n",
    "# new_train = pd.concat([clean[:14334], just_toxic])\n",
    "new_train = train\n",
    "list_sentences_train = new_train[\"cleaned\"].fillna(\"_na_\").values\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = new_train[list_classes].values\n",
    "\n",
    "\n",
    "list_extra = [\"count_sent\", \"count_word\", \"count_char\", \"count_unique_word\", \"count_punctuations\", \"count_words_upper\", \"count_words_title\", \"count_stopwords\", \"mean_word_len\", \"word_unique_percent\", \"punct_percent\",\"neg_polarity\",\"neutral_polarity\",\"positive_polarity\", \"compound_polarity\",\"misspelled_prop\",\"has_profanity\",\"profane_count\",\"profane_prop\"]\n",
    "x_extra = new_train[list_extra].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_test = test[\"cleaned\"].fillna(\"_na_\").values\n",
    "binary_y = new_train[[\"toxic\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notClean_array = 1 * ((train[\"toxic\"] == 1) | (train[\"obscene\"] == 1) | (train[\"threat\"] == 1) | (train['insult'] == 1) | (train['identity_hate'] == 1))\n",
    "clean = train[notClean_array == 0]\n",
    "just_toxic = train[notClean_array == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 19)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_extra.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
    "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
    "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
   },
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
    "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
    "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "\n",
    "inp2 = Input(shape=(3,))\n",
    "x2 = Dense(50, activation='relu')(inp2)\n",
    "\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "added = keras.layers.concatenate([x, x2])\n",
    "x = Dense(100, activation=\"relu\")(added)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[inp, inp2], outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
    "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
   },
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit([X_t, x_extra], y, batch_size=32, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "\n",
    "inp2 = Input(shape=(19,))\n",
    "x2 = Dense(50, activation='relu')(inp2)\n",
    "\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "added = keras.layers.concatenate([x, x2])\n",
    "x = Dense(100, activation=\"relu\")(added)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(added)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=[inp, inp2], outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1159s 8ms/step - loss: 0.1007 - acc: 0.9756 - val_loss: 0.0549 - val_acc: 0.9808\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1167s 8ms/step - loss: 0.0505 - acc: 0.9817 - val_loss: 0.0501 - val_acc: 0.9818\n"
     ]
    }
   ],
   "source": [
    "model.fit([X_t, x_extra], y, batch_size=32, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 47s 308us/step\n"
     ]
    }
   ],
   "source": [
    "x_extra_test = test[list_extra].values\n",
    "y_test = model.predict([X_te, x_extra_test], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[list_classes].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "\n",
    "inp2 = Input(shape=(19,))\n",
    "x2 = Dense(50, activation='relu')(inp2)\n",
    "\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "added = keras.layers.concatenate([x, x2])\n",
    "x = Dense(100, activation=\"relu\")(added)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "binary_model = Model(inputs=[inp, inp2], outputs=x)\n",
    "binary_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1104s 8ms/step - loss: 0.1945 - acc: 0.9488 - val_loss: 0.1042 - val_acc: 0.9611\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1150s 8ms/step - loss: 0.0995 - acc: 0.9637 - val_loss: 0.1007 - val_acc: 0.9627\n"
     ]
    }
   ],
   "source": [
    "binary_model.fit([X_t, x_extra], binary_y, batch_size=32, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0",
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input//dssb/sample_submission.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5525baf3827f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/dssb/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(just_toxic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtoxic_predicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtoxic_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjust_toxic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'notClean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoxic_predicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianlee/anaconda3/envs/toxic/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianlee/anaconda3/envs/toxic/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianlee/anaconda3/envs/toxic/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianlee/anaconda3/envs/toxic/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianlee/anaconda3/envs/toxic/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input//dssb/sample_submission.csv' does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_submission = pd.read_csv(f'{path}/dssb/sample_submission.csv')\n",
    "#print(just_toxic)\n",
    "toxic_predicts = np.array(y_test > 0.5) * 1\n",
    "toxic_actual = np.array(just_toxic['notClean']).reshape(len(toxic_predicts), 1)\n",
    "print(toxic_predicts)\n",
    "print(toxic_actual)\n",
    "print(abs(toxic_predicts - toxic_actual))\n",
    "misclassified_vector = abs(toxic_predicts - toxic_actual)\n",
    "\n",
    "print(np.sum(misclassified_vector == 0, axis = 0) * 1 / len(toxic_actual))\n",
    "\n",
    "#sample_submission[list_classes] = y_test\n",
    "#sample_submission.to_csv('../output/LSTM_submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
    "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251",
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
